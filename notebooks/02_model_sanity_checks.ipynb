{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee7892d-dbbb-4b53-a5fe-5c1abd0aaafb",
   "metadata": {},
   "source": [
    "# 02. Model Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2686e71-8322-4c34-8190-2f442d2efc4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394c5470-365b-4fac-b3dd-75690fc533ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "    from src.utils import load_config, setup_logging, get_device\n",
    "    from src.models import get_models # The main function we'll be testing\n",
    "    from src.data_loader import ColoredMNISTDataset # To understand target shapes\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "    print(\"Make sure your notebook is in the 'notebooks' directory of the project,\")\n",
    "    print(\"and that 'src' is a Python package (contains __init__.py) and accessible.\")\n",
    "    print(f\"Current sys.path: {sys.path}\")\n",
    "    print(f\"Attempted project_root: {project_root}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7184718-eb36-4a9e-b9c6-259fbdd8c030",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5c4240-556e-4ae3-849d-c08b68bf773a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 19:39:54 - root - INFO - Logging configured.\n",
      "2025-05-17 19:39:54 - root - INFO - Successfully loaded configuration from: /home/studio-lab-user/learning-not-to-learn/configs/colored_mnist_default.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for model checks: cpu\n"
     ]
    }
   ],
   "source": [
    "setup_logging(log_to_console=True, log_file=None)\n",
    "\n",
    "CONFIG_FILE_PATH = os.path.join(project_root, \"configs\", \"colored_mnist_default.yaml\")\n",
    "\n",
    "if not os.path.exists(CONFIG_FILE_PATH):\n",
    "    print(f\"ERROR: Configuration file not found at {CONFIG_FILE_PATH}\")\n",
    "    raise FileNotFoundError(f\"Config file missing: {CONFIG_FILE_PATH}\")\n",
    "\n",
    "config = load_config(CONFIG_FILE_PATH)\n",
    "\n",
    "# Use CPU for these sanity checks by default, unless GPU is explicitly desired for testing\n",
    "# This avoids issues if a GPU isn't available or configured for the notebook environment.\n",
    "device_name = config['training'].get('device', 'cpu')\n",
    "if device_name == 'auto': # If auto, default to CPU for notebook stability\n",
    "    device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_name)\n",
    "print(f\"Using device for model checks: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c1d3f9-cd2b-4441-931a-3d40ec4b8645",
   "metadata": {},
   "source": [
    "#### 1. Instantiate Models using get_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735fc2e2-3cf5-42b3-8ebc-3d39e077e1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 19:40:36 - root - INFO - LeNet_F initialized: Input Channels=3, Feature Dim=128\n",
      "2025-05-17 19:40:36 - root - INFO - MLP_Classifier_G initialized: Input Dim=128, Hidden Dims=[64], Output Dim=10\n",
      "2025-05-17 19:40:36 - root - INFO - ConvBiasPredictorH initialized: InputFeatureDim=128, NumBiasChannels=3, NumBiasQuantizationBins=8, Output Spatial=(14,14), IntermediateChannels=64, BaseSpatialDimForUpsample=7\n",
      "2025-05-17 19:40:36 - root - INFO - All models created and moved to device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Instantiating Models ---\n",
      "Models instantiated successfully:\n",
      "  Feature Extractor (f): LeNet_F\n",
      "  Task Classifier (g): MLP_Classifier_G\n",
      "  Bias Predictor (h): ConvBiasPredictorH\n",
      "  Gradient Reversal Layer (grl): GradientReversalLayer with lambda=1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 1. Instantiating Models ---\")\n",
    "try:\n",
    "    models_dict = get_models(config, device)\n",
    "    feature_extractor_f = models_dict['feature_extractor_f']\n",
    "    task_classifier_g = models_dict['task_classifier_g']\n",
    "    bias_predictor_h = models_dict['bias_predictor_h']\n",
    "    grl_layer = models_dict['grl'] # Gradient Reversal Layer\n",
    "\n",
    "    print(\"Models instantiated successfully:\")\n",
    "    print(f\"  Feature Extractor (f): {type(feature_extractor_f).__name__}\")\n",
    "    print(f\"  Task Classifier (g): {type(task_classifier_g).__name__}\")\n",
    "    print(f\"  Bias Predictor (h): {type(bias_predictor_h).__name__}\")\n",
    "    print(f\"  Gradient Reversal Layer (grl): {type(grl_layer).__name__} with lambda={grl_layer.lambda_val}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to instantiate models using get_models: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be2f34-3d6c-4b02-acda-b389375eb29f",
   "metadata": {},
   "source": [
    "#### 2. Prepare Dummy Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e561b7-979e-463a-8419-5fc7ab8cb697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Preparing Dummy Inputs ---\n",
      "Dummy images shape: torch.Size([4, 3, 28, 28]) on device: cpu\n",
      "Dummy features shape: torch.Size([4, 128]) on device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2. Preparing Dummy Inputs ---\")\n",
    "# Get parameters from config for shapes\n",
    "data_cfg = config['data']\n",
    "model_cfg = config['model']\n",
    "\n",
    "batch_size = 4 # A small batch for testing\n",
    "img_channels = data_cfg.get('img_channels', 3)\n",
    "img_h = data_cfg.get('img_size', 28)\n",
    "img_w = data_cfg.get('img_size', 28)\n",
    "\n",
    "feature_dim = model_cfg['feature_extractor_f']['params']['feature_dim']\n",
    "\n",
    "# Dummy image batch (like from DataLoader)\n",
    "dummy_images = torch.randn(batch_size, img_channels, img_h, img_w).to(device)\n",
    "print(f\"Dummy images shape: {dummy_images.shape} on device: {dummy_images.device}\")\n",
    "\n",
    "# Dummy feature vector (output of f, input to g and h)\n",
    "dummy_features = torch.randn(batch_size, feature_dim).to(device)\n",
    "print(f\"Dummy features shape: {dummy_features.shape} on device: {dummy_features.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28313618-c07c-449f-834d-4f9828617949",
   "metadata": {},
   "source": [
    "#### 3. Test Forward Pass of Feature Extractor ( $f$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20683e31-2c65-4ad9-93ad-44b39ee8638f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Testing Feature Extractor (f) ---\n",
      "Output features shape from f: torch.Size([4, 128])\n",
      "Feature Extractor (f) forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 3. Testing Feature Extractor (f) ---\")\n",
    "try:\n",
    "    feature_extractor_f.eval() # Set to eval mode for sanity check\n",
    "    with torch.no_grad(): # No need to compute gradients\n",
    "        output_features_f = feature_extractor_f(dummy_images)\n",
    "    print(f\"Output features shape from f: {output_features_f.shape}\")\n",
    "    assert output_features_f.shape == (batch_size, feature_dim), \\\n",
    "        f\"Expected feature_dim {feature_dim}, got {output_features_f.shape[1]}\"\n",
    "    print(\"Feature Extractor (f) forward pass successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during Feature Extractor (f) forward pass: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad577a-a46a-4612-a7e2-e7132892f5d4",
   "metadata": {},
   "source": [
    "#### 4. Test Forward Pass of Task Classifier ( $g$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93e438bb-b129-4455-9e7f-f742dfda5455",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Testing Task Classifier (g) ---\n",
      "Output logits shape from g: torch.Size([4, 10])\n",
      "Task Classifier (g) forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 4. Testing Task Classifier (g) ---\")\n",
    "num_main_classes = data_cfg['num_main_classes']\n",
    "try:\n",
    "    task_classifier_g.eval()\n",
    "    with torch.no_grad():\n",
    "        # Use features from f's output for a more integrated test, or dummy_features for isolation\n",
    "        output_task_g = task_classifier_g(output_features_f) \n",
    "        # output_task_g = task_classifier_g(dummy_features) # Alternative for isolated test\n",
    "    print(f\"Output logits shape from g: {output_task_g.shape}\")\n",
    "    assert output_task_g.shape == (batch_size, num_main_classes), \\\n",
    "        f\"Expected num_main_classes {num_main_classes}, got {output_task_g.shape[1]}\"\n",
    "    print(\"Task Classifier (g) forward pass successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during Task Classifier (g) forward pass: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e643b7-6a6c-4455-adca-2e2027277563",
   "metadata": {},
   "source": [
    "#### 5. Test Forward Pass of Bias Predictor ( $h$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97997464-6222-48bc-b3f5-fd708f1f1625",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Testing Bias Predictor (h) ---\n",
      "Output bias prediction shape from h: torch.Size([4, 8, 3, 14, 14])\n",
      "Bias Predictor (h) forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 5. Testing Bias Predictor (h) ---\")\n",
    "# Expected output shape for ConvBiasPredictorH:\n",
    "# (Batch, NumBiasBins, NumBiasChannels, H_out, W_out)\n",
    "h_params = model_cfg['bias_predictor_h']['params']\n",
    "num_bias_quant_bins = h_params['num_bias_quantization_bins']\n",
    "# num_bias_channels is effectively img_channels for our setup\n",
    "h_output_h = h_params['output_h']\n",
    "h_output_w = h_params['output_w']\n",
    "expected_h_shape = (batch_size, num_bias_quant_bins, img_channels, h_output_h, h_output_w)\n",
    "\n",
    "try:\n",
    "    bias_predictor_h.eval()\n",
    "    with torch.no_grad():\n",
    "        # Use features from f's output\n",
    "        output_bias_h = bias_predictor_h(output_features_f)\n",
    "        # output_bias_h = bias_predictor_h(dummy_features) # Alternative for isolated test\n",
    "    print(f\"Output bias prediction shape from h: {output_bias_h.shape}\")\n",
    "    assert output_bias_h.shape == expected_h_shape, \\\n",
    "        f\"Expected shape {expected_h_shape}, got {output_bias_h.shape}\"\n",
    "    print(\"Bias Predictor (h) forward pass successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during Bias Predictor (h) forward pass: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14638c5-d933-4847-9db5-947fa5520a90",
   "metadata": {},
   "source": [
    "#### 6. Test Gradient Reversal Layer (GRL)\n",
    "GRL's main effect is on the backward pass, but we can check its forward pass (identity) and that it can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a10bca8a-a833-494a-9516-195957ddee3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Testing Gradient Reversal Layer (GRL) ---\n",
      "Shape after GRL forward pass: torch.Size([4, 128])\n",
      "GRL forward pass behaves as identity (shape check).\n",
      "Re-running f for clearer GRL backward demo...\n",
      "Gradient on input to GRL (sample from re-run): tensor([-1., -1., -1., -1., -1.])\n",
      "Note: This gradient should be negative of what it would be without GRL (scaled by lambda=1.0).\n",
      "GRL backward pass conceptually checked.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 6. Testing Gradient Reversal Layer (GRL) ---\")\n",
    "try:\n",
    "    # GRL doesn't have eval/train mode\n",
    "    # It's a torch.autograd.Function wrapper\n",
    "    features_through_grl = grl_layer(output_features_f.clone().requires_grad_(True)) # Need requires_grad for backward demo\n",
    "    print(f\"Shape after GRL forward pass: {features_through_grl.shape}\")\n",
    "    assert features_through_grl.shape == output_features_f.shape, \"GRL forward pass should be identity.\"\n",
    "    print(\"GRL forward pass behaves as identity (shape check).\")\n",
    "\n",
    "    # Conceptual check of backward pass (requires a subsequent layer and loss)\n",
    "    if features_through_grl.requires_grad:\n",
    "        # Simulate a dummy loss and backward pass on GRL's output\n",
    "        dummy_downstream_loss = features_through_grl.mean() \n",
    "        dummy_downstream_loss.backward()\n",
    "        \n",
    "        # Check if the original features (before GRL) have gradients\n",
    "        # The gradient should be reversed and scaled by GRL's lambda\n",
    "        if output_features_f.grad is not None:\n",
    "            print(f\"Gradient on input to GRL (sample): {output_features_f.grad[0, :5]}\")\n",
    "            print(\"GRL backward pass conceptually checked (gradient exists).\")\n",
    "        else:\n",
    "            # This might happen if output_features_f itself didn't require grad or was detached.\n",
    "            # Let's re-run f with requires_grad on input for a clearer GRL backward demo\n",
    "            print(\"Re-running f for clearer GRL backward demo...\")\n",
    "            dummy_images_grad = torch.randn(batch_size, img_channels, img_h, img_w, device=device, requires_grad=True)\n",
    "            feature_extractor_f.train() # Ensure params have grads\n",
    "            output_features_f_grad = feature_extractor_f(dummy_images_grad)\n",
    "            \n",
    "            # Detach for GRL input to isolate GRL's effect on its input's grad, not f's params\n",
    "            output_features_f_grad_detached = output_features_f_grad.detach().requires_grad_(True) \n",
    "            \n",
    "            features_through_grl_2 = grl_layer(output_features_f_grad_detached)\n",
    "            dummy_downstream_loss_2 = features_through_grl_2.sum() # Use sum for non-zero gradients\n",
    "            dummy_downstream_loss_2.backward()\n",
    "            \n",
    "            if output_features_f_grad_detached.grad is not None:\n",
    "                print(f\"Gradient on input to GRL (sample from re-run): {output_features_f_grad_detached.grad[0, :5]}\")\n",
    "                print(f\"Note: This gradient should be negative of what it would be without GRL (scaled by lambda={grl_layer.lambda_val}).\")\n",
    "                print(\"GRL backward pass conceptually checked.\")\n",
    "            else:\n",
    "                print(\"WARN: Still no gradient on GRL input after re-run. Check requires_grad flags.\")\n",
    "    else:\n",
    "        print(\"WARN: Output of GRL does not require grad. Cannot test backward pass effect directly here.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ERROR during GRL test: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d983db-54de-4359-8563-b12b6d3ba9e2",
   "metadata": {},
   "source": [
    "#### 7. Conceptual Loss Application (Shape Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c7f50e4-3ba5-4c6e-bccd-f87ab8be76da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Conceptual Loss Application (Shape Check) ---\n",
      "Conceptual main task loss (g) calculated: 2.2464\n",
      "Conceptual bias prediction loss (h) calculated: 2.0811\n",
      "\n",
      "Model sanity checks notebook finished.\n",
      "If all assertions passed and no errors occurred, your model architectures and forward passes are likely correct.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 7. Conceptual Loss Application (Shape Check) ---\")\n",
    "# Main Task Loss (g)\n",
    "num_main_classes = data_cfg['num_main_classes']\n",
    "dummy_main_labels = torch.randint(0, num_main_classes, (batch_size,), device=device, dtype=torch.long)\n",
    "criterion_main = torch.nn.CrossEntropyLoss()\n",
    "try:\n",
    "    loss_g = criterion_main(output_task_g, dummy_main_labels)\n",
    "    print(f\"Conceptual main task loss (g) calculated: {loss_g.item():.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR calculating conceptual main task loss: {e}\")\n",
    "\n",
    "# Bias Prediction Loss (h)\n",
    "# Target shape for bias from data_loader: (B, C, H_out, W_out) with class indices [0, NumBiasBins-1]\n",
    "# Bias predictor h output shape: (B, NumBiasBins, C, H_out, W_out)\n",
    "dummy_bias_targets = torch.randint(0, num_bias_quant_bins, \n",
    "                                   (batch_size, img_channels, h_output_h, h_output_w), \n",
    "                                   device=device, dtype=torch.long)\n",
    "criterion_bias = torch.nn.CrossEntropyLoss(ignore_index=255) # As used in Trainer\n",
    "try:\n",
    "    loss_h = criterion_bias(output_bias_h, dummy_bias_targets)\n",
    "    print(f\"Conceptual bias prediction loss (h) calculated: {loss_h.item():.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR calculating conceptual bias prediction loss: {e}\")\n",
    "    print(\"  Check shapes: output_bias_h (preds) vs dummy_bias_targets (targets)\")\n",
    "    print(f\"  output_bias_h shape: {output_bias_h.shape}\")\n",
    "    print(f\"  dummy_bias_targets shape: {dummy_bias_targets.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\nModel sanity checks notebook finished.\")\n",
    "print(\"If all assertions passed and no errors occurred, your model architectures and forward passes are likely correct.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
