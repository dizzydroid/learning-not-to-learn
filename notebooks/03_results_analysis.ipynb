{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e9b8d-e402-4f62-b8ac-099a5b6e05d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from tensorboard.backend.event_processing import event_accumulator # For reading TensorBoard logs\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef8392-f18e-496c-b8b9-f6fd02853a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "    from src.utils import load_config, setup_logging, get_device\n",
    "    from src.models import get_models\n",
    "    from src.data_loader import get_data_loaders\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "    print(\"Make sure your notebook is in the 'notebooks' directory of the project,\")\n",
    "    print(\"and that 'src' is a Python package (contains __init__.py) and accessible.\")\n",
    "    print(f\"Current sys.path: {sys.path}\")\n",
    "    print(f\"Attempted project_root: {project_root}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4d6f4-2e44-459e-939d-0a03313a74f5",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeaeba0-762e-4f1d-b53f-4d1f53b2d6b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setup_logging(log_to_console=True, log_file=None)\n",
    "\n",
    "# IMPORTANT: Update these paths to point to your specific experiment results\n",
    "EXPERIMENT_NAME = \"colored_mnist_baseline\" # Change this to your actual experiment name\n",
    "CONFIG_FILE_PATH = os.path.join(project_root, \"configs\", \"colored_mnist_default.yaml\") # Or the config used for the experiment\n",
    "RESULTS_BASE_DIR = os.path.join(project_root, \"results\")\n",
    "EXPERIMENT_DIR = os.path.join(RESULTS_BASE_DIR, EXPERIMENT_NAME)\n",
    "TENSORBOARD_LOG_DIR = os.path.join(EXPERIMENT_DIR, \"tensorboard_logs\")\n",
    "CHECKPOINTS_DIR = os.path.join(EXPERIMENT_DIR, \"checkpoints\")\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINTS_DIR, \"best_model.pth\") # Or specific checkpoint\n",
    "\n",
    "if not os.path.exists(EXPERIMENT_DIR):\n",
    "    print(f\"ERROR: Experiment directory not found: {EXPERIMENT_DIR}\")\n",
    "    print(\"Please ensure you have run training and specified the correct EXPERIMENT_NAME.\")\n",
    "    # Stop execution or handle gracefully\n",
    "    raise FileNotFoundError(f\"Experiment directory missing: {EXPERIMENT_DIR}\")\n",
    "\n",
    "config = load_config(CONFIG_FILE_PATH)\n",
    "device = get_device(config['training'].get('device', 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1073b4c-d884-44aa-8101-0aecbf57423e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. Load Metrics from TensorBoard Logs ---\n",
    "print(f\"\\n--- 1. Loading Metrics from TensorBoard Logs: {TENSORBOARD_LOG_DIR} ---\")\n",
    "\n",
    "def load_tensorboard_metrics(log_dir):\n",
    "    \"\"\"Loads scalar metrics from TensorBoard event files.\"\"\"\n",
    "    ea = event_accumulator.EventAccumulator(log_dir,\n",
    "        size_guidance={ # See EA docs for more options\n",
    "            event_accumulator.SCALARS: 0, # 0 = load all\n",
    "        })\n",
    "    ea.Reload() # Load all data from events files\n",
    "    \n",
    "    metrics = {}\n",
    "    tags = ea.Tags()['scalars']\n",
    "    for tag in tags:\n",
    "        events = ea.Scalars(tag)\n",
    "        metrics[tag] = pd.DataFrame([(event.step, event.value) for event in events],\n",
    "                                    columns=['step', 'value'])\n",
    "    if not metrics:\n",
    "        print(f\"No scalar metrics found in {log_dir}. Ensure TensorBoard logging was enabled and training ran.\")\n",
    "    return metrics\n",
    "\n",
    "tb_metrics = {}\n",
    "if os.path.exists(TENSORBOARD_LOG_DIR) and len(os.listdir(TENSORBOARD_LOG_DIR)) > 0 :\n",
    "    tb_metrics = load_tensorboard_metrics(TENSORBOARD_LOG_DIR)\n",
    "    print(f\"Available metrics from TensorBoard: {list(tb_metrics.keys())}\")\n",
    "else:\n",
    "    print(f\"TensorBoard log directory is empty or does not exist: {TENSORBOARD_LOG_DIR}\")\n",
    "    print(\"Skipping TensorBoard metrics loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da475f89-407a-472f-ac5a-2424c0566f89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 2. Plot Learning Curves ---\n",
    "print(\"\\n--- 2. Plotting Learning Curves ---\")\n",
    "\n",
    "def plot_metric(metrics_df_dict, primary_metric_key, val_metric_key=None, title=\"Metric over Epochs\", ax=None):\n",
    "    \"\"\"Plots a training metric and its corresponding validation metric.\"\"\"\n",
    "    show_plot_later = False\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        show_plot_later = True\n",
    "\n",
    "    if primary_metric_key in metrics_df_dict:\n",
    "        df_train = metrics_df_dict[primary_metric_key]\n",
    "        ax.plot(df_train['step'], df_train['value'], label=f'Train {primary_metric_key.split(\"/\")[-1]}', alpha=0.8)\n",
    "    else:\n",
    "        print(f\"Warning: Metric '{primary_metric_key}' not found in loaded TensorBoard metrics.\")\n",
    "\n",
    "    if val_metric_key and val_metric_key in metrics_df_dict:\n",
    "        df_val = metrics_df_dict[val_metric_key]\n",
    "        ax.plot(df_val['step'], df_val['value'], label=f'Validation {val_metric_key.split(\"/\")[-1]}', marker='o', linestyle='--')\n",
    "    elif val_metric_key:\n",
    "        print(f\"Warning: Metric '{val_metric_key}' not found for validation.\")\n",
    "        \n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(title.split(' ')[0]) # Use first word of title as y-label\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    if show_plot_later:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "if tb_metrics:\n",
    "    # Plot Main Task Accuracy\n",
    "    plot_metric(tb_metrics, 'Train/accuracy_g', 'Validation/accuracy_g', 'Main Task Accuracy (G)')\n",
    "    \n",
    "    # Plot Main Task Loss\n",
    "    plot_metric(tb_metrics, 'Train/loss_g', 'Validation/loss_g', 'Main Task Loss (G)')\n",
    "\n",
    "    # Plot Bias Predictor (H) Direct Accuracy\n",
    "    plot_metric(tb_metrics, 'Train/accuracy_h_direct', 'Validation/accuracy_h', 'Bias Predictor Accuracy (H - Direct)')\n",
    "    \n",
    "    # Plot Bias Predictor (H) Direct Loss\n",
    "    plot_metric(tb_metrics, 'Train/loss_h_direct', 'Validation/loss_h', 'Bias Predictor Loss (H - Direct)')\n",
    "\n",
    "    # Plot Adversarial Loss for F\n",
    "    if 'Train/loss_adv_for_f' in tb_metrics:\n",
    "         plot_metric(tb_metrics, 'Train/loss_adv_for_f', title='Adversarial Loss for F')\n",
    "    \n",
    "    # Plot Accuracy of H on GRL features (how well F is fooling H)\n",
    "    if 'Train/accuracy_h_adv' in tb_metrics:\n",
    "        plot_metric(tb_metrics, 'Train/accuracy_h_adv', title='Bias Predictor Accuracy (H - Adversarial Path)')\n",
    "else:\n",
    "    print(\"Skipping plotting learning curves as no TensorBoard metrics were loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a304e7-bc22-4050-87f5-a4286e934d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 3. Load Model and Evaluate on Test Set (Example) ---\n",
    "# This section shows how to load the best model and get predictions for further analysis.\n",
    "print(\"\\n--- 3. Load Best Model and Evaluate (Example) ---\")\n",
    "\n",
    "if not os.path.exists(BEST_MODEL_PATH):\n",
    "    print(f\"Best model checkpoint not found at: {BEST_MODEL_PATH}\")\n",
    "    print(\"Skipping model loading and detailed evaluation.\")\n",
    "else:\n",
    "    print(f\"Loading best model from: {BEST_MODEL_PATH}\")\n",
    "    models_dict_eval = get_models(config, device) # Re-initialize model structures\n",
    "    \n",
    "    # Create a deep copy of the config for this section to avoid modifying the global one\n",
    "    eval_config = type(config)(config) # Creates a shallow copy for dicts; use copy.deepcopy for true isolation if needed\n",
    "\n",
    "    if 'data' in eval_config and 'path' in eval_config['data']:\n",
    "        if not os.path.isabs(eval_config['data']['path']):\n",
    "            absolute_data_path = os.path.join(project_root, eval_config['data']['path'])\n",
    "            eval_config['data']['path'] = os.path.abspath(absolute_data_path)\n",
    "            print(f\"NOTEBOOK (Evaluation Cell): Updated eval_config['data']['path'] to absolute: {eval_config['data']['path']}\")\n",
    "    else:\n",
    "        print(\"WARNING (Evaluation Cell): 'data' or 'data.path' not found in eval_config.\")\n",
    "\n",
    "    models_dict_eval = get_models(eval_config, device) # Re-initialize model structures\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(BEST_MODEL_PATH, map_location=device)\n",
    "        models_dict_eval['feature_extractor_f'].load_state_dict(checkpoint['feature_extractor_f_state_dict'])\n",
    "        models_dict_eval['task_classifier_g'].load_state_dict(checkpoint['task_classifier_g_state_dict'])\n",
    "        # models_dict_eval['bias_predictor_h'].load_state_dict(checkpoint['bias_predictor_h_state_dict']) # Optional for this part\n",
    "        \n",
    "        f_net_eval = models_dict_eval['feature_extractor_f'].eval()\n",
    "        g_net_eval = models_dict_eval['task_classifier_g'].eval()\n",
    "        \n",
    "        print(\"Model loaded successfully for evaluation.\")\n",
    "\n",
    "        # Get Test DataLoader\n",
    "        # Ensure create_val_loader is False if you only want the original test set\n",
    "        # Or adjust get_data_loaders to always return a test_loader regardless of val split logic for train\n",
    "        _, test_loader, _ = get_data_loaders(config, create_val_loader=False) \n",
    "\n",
    "        all_main_labels = []\n",
    "        all_main_preds = []\n",
    "        # all_bias_targets = [] # If you want to analyze bias predictions\n",
    "        # all_bias_preds_map = []\n",
    "\n",
    "        print(\"Running model on test set to collect predictions...\")\n",
    "        with torch.no_grad():\n",
    "            for images, bias_targets_batch, main_labels_batch in test_loader:\n",
    "                images = images.to(device)\n",
    "                main_labels_batch = main_labels_batch.to(device)\n",
    "                # bias_targets_batch = bias_targets_batch.to(device)\n",
    "\n",
    "                features = f_net_eval(images)\n",
    "                task_outputs = g_net_eval(features)\n",
    "                _, main_preds_batch = torch.max(task_outputs, 1)\n",
    "                \n",
    "                all_main_labels.extend(main_labels_batch.cpu().numpy())\n",
    "                all_main_preds.extend(main_preds_batch.cpu().numpy())\n",
    "\n",
    "                # bias_preds_h = models_dict_eval['bias_predictor_h'](features)\n",
    "                # _, bias_preds_map_batch = torch.max(bias_preds_h, dim=1) # (B, C, H, W)\n",
    "                # all_bias_targets.extend(bias_targets_batch.cpu().numpy())\n",
    "                # all_bias_preds_map.extend(bias_preds_map_batch.cpu().numpy())\n",
    "        \n",
    "        all_main_labels = np.array(all_main_labels)\n",
    "        all_main_preds = np.array(all_main_preds)\n",
    "\n",
    "        # --- 4. Confusion Matrix for Main Task ---\n",
    "        print(\"\\n--- 4. Confusion Matrix for Main Task (on Test Set) ---\")\n",
    "        if len(all_main_labels) > 0:\n",
    "            cm = confusion_matrix(all_main_labels, all_main_preds)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                        xticklabels=range(config['data']['num_main_classes']), \n",
    "                        yticklabels=range(config['data']['num_main_classes']))\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.title('Confusion Matrix - Main Task')\n",
    "            plt.show()\n",
    "            \n",
    "            test_accuracy = np.mean(all_main_labels == all_main_preds)\n",
    "            print(f\"Overall Test Accuracy (from collected predictions): {test_accuracy:.4f}\")\n",
    "        else:\n",
    "            print(\"No predictions collected to generate confusion matrix.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Could not load checkpoint for evaluation: {BEST_MODEL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model loading or evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04cebb-13da-4ae2-a690-5fd8ffb17380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 5. Further Analysis (Placeholders) ---\n",
    "print(\"\\n--- 5. Further Analysis Ideas (Placeholders) ---\")\n",
    "print(\"- Analyze performance on bias-conflicting vs. bias-aligned samples (requires dataset to be structured for this).\")\n",
    "print(\"- Visualize features from f_net (e.g., using t-SNE) colored by main label and by bias label.\")\n",
    "print(\"- Examine misclassifications: what kind of samples are hard for the debiased model?\")\n",
    "print(\"- If bias is image-level (not pixel-wise), compare accuracy of H on test set.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
